import json
import re
import os
from openai import OpenAI
from tqdm import tqdm

# --------- vLLM OpenAI API å®¢æˆ·ç«¯é…ç½® ---------
client = OpenAI(
    api_key="EMPTY",  # vLLMä¸éœ€è¦çœŸå®çš„API key
    base_url="http://localhost:8000/v1"
)
model_name = "llama8b"  # å¯¹åº”start_vllm_server.shä¸­è®¾ç½®çš„served-model-name

# --------- Prompt æ¨¡æ¿ ---------
STAGE1_PROMPT = """
You are an extractor. Input: the chunk delimited by triple backticks.
Task: Return a JSON array (only the array, nothing else) of exact strings that must later appear verbatim in at least one question.
Include:
  - every named entity (person, organization, location, title, product, book, etc.) exactly as written;
  - every descriptive phrase that identifies an object (e.g., "a wide river", "an ancient temple") exactly as written;
  - any short atomic factual phrases (phrases that express a single fact) that the next stage must cover.
Do NOT add, paraphrase, split, or invent. Preserve case and punctuation exactly. If none, return [].
Example input:
```Alexander Fleming discovered penicillin in 1928 in London.```
Output:
["Alexander Fleming","penicillin","1928","London"]

Now extract REQUIRED_PHRASES for:
```{chunk}```
"""

STAGE2_PROMPT = """
You are a question generation assistant. Generate questions and answers based on the given text.

Task: Create a JSON array where each element is an object with "question" and "answer" fields.

Rules:
1. Each question must include exactly ONE phrase from REQUIRED_PHRASES
2. Avoid pronouns like "this", "that", "it", "they", "he", "she", etc.
3. Use specific names and phrases from REQUIRED_PHRASES
4. Each answer must be copied exactly from the text
5. Cover all REQUIRED_PHRASES in your questions
6. Output only the JSON array in a code block

Example:
Text: "Alexander Fleming discovered penicillin in 1928 in London."
REQUIRED_PHRASES: ["Alexander Fleming", "penicillin", "1928", "London"]

Output:
```json
[
  {{"question": "Who discovered penicillin?", "answer": "Alexander Fleming discovered penicillin"}},
  {{"question": "When was penicillin discovered?", "answer": "Penicillin discovered in1928"}},
  {{"question": "Where was penicillin discovered?", "answer": "Penicillin was discovered in London"}}
]
```

Now generate questions for:
Text: ```{chunk}```
REQUIRED_PHRASES: {required_phrases}
"""

# --------- è¾…åŠ©å‡½æ•° ---------
BAD_WORDS = [
"this","that","it","they","those","these",
 "he","she","his","her","him","their","them","we","us","I",
 "the film","this film","the director","the book","the article","the passage","the story","the movie",
 "the person","the place","the country","the year",
 "the director","this director","the book","this book","the article","this article","the passage","this passage","the story","this story","the movie","this movie",
 "the person","this person","the place","this place","the country","this country","the year","this year"
]

def has_bad_word(s):
    s_low = s.lower()
    return any(w in s_low for w in BAD_WORDS)

def coverage_check(required_phrases, qa_list):
    questions = [q["question"] for q in qa_list]
    missing = [p for p in required_phrases if not any(p in q for q in questions)]
    return missing

def call_llm(prompt, max_new_tokens=512, max_retries=3):
    """
    è°ƒç”¨vLLM APIç”Ÿæˆæ–‡æœ¬ï¼Œè¿”å›ç”Ÿæˆç»“æœå’Œtokenç»Ÿè®¡ä¿¡æ¯
    åŒ…å«é‡è¯•æœºåˆ¶ç¡®ä¿ç¨³å®šæ€§
    """
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_new_tokens,
                temperature=0.0,
                stream=False
            )
            
            generated_text = response.choices[0].message.content
            input_tokens = response.usage.prompt_tokens
            output_tokens = response.usage.completion_tokens
            total_tokens = response.usage.total_tokens
            
            return {
                "text": generated_text,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": total_tokens
            }
        except Exception as e:
            print(f"âš ï¸  APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                print(f"ğŸ”„ ç­‰å¾…2ç§’åé‡è¯•...")
                import time
                time.sleep(2)
            else:
                print(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¿”å›ç©ºç»“æœ")
                return {
                    "text": "",
                    "input_tokens": 0,
                    "output_tokens": 0,
                    "total_tokens": 0
                }

def extract_json_block(text):
    match = re.search(r"```json(.*?)```", text, re.S)
    if match:
        return match.group(1).strip()
    return text.strip()

# --------- ä¸»æµç¨‹ ---------
def generate_qa(chunk):
    """
    ç”ŸæˆQAå¯¹ï¼Œè¿”å›QAåˆ—è¡¨å’Œtokenç»Ÿè®¡ä¿¡æ¯
    """
    total_input_tokens = 0
    total_output_tokens = 0
    total_tokens = 0
    
    # Stage 1
    stage1_prompt = STAGE1_PROMPT.format(chunk=chunk)
    stage1_result = call_llm(stage1_prompt, max_new_tokens=256)
    total_input_tokens += stage1_result["input_tokens"]
    total_output_tokens += stage1_result["output_tokens"]
    total_tokens += stage1_result["total_tokens"]
    
    try:
        required_phrases = json.loads(extract_json_block(stage1_result["text"]))
    except:
        required_phrases = []
    print("Stage1 REQUIRED_PHRASES:", required_phrases)

    # Stage 2
    stage2_prompt = STAGE2_PROMPT.format(chunk=chunk, required_phrases=json.dumps(required_phrases))
    stage2_result = call_llm(stage2_prompt, max_new_tokens=1024)
    total_input_tokens += stage2_result["input_tokens"]
    total_output_tokens += stage2_result["output_tokens"]
    total_tokens += stage2_result["total_tokens"]
    
    qa_text = extract_json_block(stage2_result["text"])
    try:
        qa_list = json.loads(qa_text)
    except:
        qa_list = []

    # æœ¬åœ°æ ¡éªŒ
    invalid = []
    for qa in qa_list:
        if has_bad_word(qa["question"]):
            invalid.append(qa)
    missing = coverage_check(required_phrases, qa_list)

    if invalid or missing:
        print("æ ¡éªŒå¤±è´¥ï¼Œé‡ç”Ÿç¼ºå¤±éƒ¨åˆ†...")
        print("Invalid questions:", invalid)
        print("Missing phrases:", missing)

    return {
        "qa_list": qa_list,
        "input_tokens": total_input_tokens,
        "output_tokens": total_output_tokens,
        "total_tokens": total_tokens,
        "validation_info": {
            "invalid_questions": invalid,
            "missing_phrases": missing,
            "required_phrases": required_phrases
        }
    }

# --------- è¯­æ–™å¤„ç†å‡½æ•° ---------
def process_corpus_file(input_file, output_file, dataset_name):
    """
    å¤„ç†å•ä¸ªè¯­æ–™æ–‡ä»¶ï¼Œç”ŸæˆQAå¹¶å¢é‡ä¿å­˜
    """
    print(f"å¼€å§‹å¤„ç† {dataset_name} æ•°æ®é›†...")
    print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    # åˆ›å»ºå¤±è´¥æ ·æœ¬ä¿å­˜æ–‡ä»¶
    failed_file = output_file.replace('.jsonl', '_failed.jsonl')
    print(f"å¤±è´¥æ ·æœ¬å°†ä¿å­˜åˆ°: {failed_file}")
    
    # è¯»å–è¯­æ–™æ–‡ä»¶
    corpus_data = []
    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                corpus_data.append(json.loads(line.strip()))
    
    print(f"æ€»å…±è¯»å– {len(corpus_data)} æ¡è¯­æ–™")
    
    # æ£€æŸ¥å·²å¤„ç†çš„è¯­æ–™æ•°é‡ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    processed_count = 0
    if os.path.exists(output_file):
        with open(output_file, 'r', encoding='utf-8') as f:
            processed_count = sum(1 for line in f if line.strip())
        print(f"å‘ç°å·²å¤„ç† {processed_count} æ¡è¯­æ–™ï¼Œå°†ä»ç¬¬ {processed_count + 1} æ¡å¼€å§‹")
    
    # ç»Ÿè®¡å˜é‡
    total_input_tokens = 0
    total_output_tokens = 0
    total_total_tokens = 0
    total_qa_count = 0
    failed_count = 0
    
    # ä»æ–­ç‚¹å¼€å§‹å¤„ç†
    for i in range(processed_count, len(corpus_data)):
        item = corpus_data[i]
        title = item.get("title", "")
        passage = item.get("passage", "")
        
        print(f"\nğŸ”„ å¤„ç†ç¬¬ {i + 1}/{len(corpus_data)} æ¡è¯­æ–™: {title[:50]}...")
        
        try:
            # ä½¿ç”¨passageä½œä¸ºchunkç”ŸæˆQA
            result = generate_qa(passage)
            
            # æ£€æŸ¥ç”Ÿæˆç»“æœæ˜¯å¦æœ‰æ•ˆ
            if not result["qa_list"] or len(result["qa_list"]) == 0:
                print(f"âš ï¸  ç¬¬ {i + 1} æ¡è¯­æ–™æœªç”Ÿæˆæœ‰æ•ˆQAå¯¹ï¼Œä¿å­˜ä¸ºå¤±è´¥æ ·æœ¬")
                failed_count += 1
                
                # ä¿å­˜å¤±è´¥æ ·æœ¬
                failed_entry = {
                    "index": i + 1,
                    "title": title,
                    "passage": passage,
                    "reason": "no_valid_qa_generated",
                    "qa_list": result["qa_list"],
                    "input_tokens": result["input_tokens"],
                    "output_tokens": result["output_tokens"],
                    "total_tokens": result["total_tokens"],
                    "validation_info": result.get("validation_info", {})
                }
                
                with open(failed_file, 'a', encoding='utf-8') as f:
                    f.write(json.dumps(failed_entry, ensure_ascii=False) + '\n')
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯é—®é¢˜ä½†ä»æœ‰QAå¯¹çš„æƒ…å†µ
            validation_info = result.get("validation_info", {})
            has_validation_issues = (validation_info.get("invalid_questions") or 
                                   validation_info.get("missing_phrases"))
            
            if has_validation_issues:
                print(f"âš ï¸  ç¬¬ {i + 1} æ¡è¯­æ–™æœ‰éªŒè¯é—®é¢˜ï¼Œä½†ä»ä¿å­˜QAå¯¹")
                # åœ¨æˆåŠŸæ ·æœ¬ä¸­ä¹Ÿè®°å½•éªŒè¯ä¿¡æ¯
                corpus_entry = {
                    "title": title,
                    "passage": passage,
                    "qa": result["qa_list"],
                    "input_tokens": result["input_tokens"],
                    "output_tokens": result["output_tokens"],
                    "total_tokens": result["total_tokens"],
                    "validation_warnings": validation_info
                }
            else:
                # æ­£å¸¸çš„æˆåŠŸæ ·æœ¬
                corpus_entry = {
                    "title": title,
                    "passage": passage,
                    "qa": result["qa_list"],
                    "input_tokens": result["input_tokens"],
                    "output_tokens": result["output_tokens"],
                    "total_tokens": result["total_tokens"]
                }
            
            
            # ç«‹å³ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆå¢é‡ä¿å­˜ï¼‰
            with open(output_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(corpus_entry, ensure_ascii=False) + '\n')
            
            # ç´¯è®¡ç»Ÿè®¡
            total_input_tokens += result["input_tokens"]
            total_output_tokens += result["output_tokens"]
            total_total_tokens += result["total_tokens"]
            total_qa_count += len(result["qa_list"])
            
            print(f"âœ… æˆåŠŸç”Ÿæˆ {len(result['qa_list'])} ä¸ªQAå¯¹ï¼Œå·²ä¿å­˜")
            print(f"ğŸ“Š å½“å‰ç»Ÿè®¡: æ€»QAå¯¹={total_qa_count}, æ€»token={total_total_tokens}")
            
        except Exception as e:
            print(f"âŒ å¤„ç†ç¬¬ {i + 1} æ¡è¯­æ–™æ—¶å‡ºé”™: {e}")
            print(f"ä¿å­˜ä¸ºå¤±è´¥æ ·æœ¬ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€æ¡...")
            failed_count += 1
            
            # ä¿å­˜å¤±è´¥æ ·æœ¬
            failed_entry = {
                "index": i + 1,
                "title": title,
                "passage": passage,
                "reason": f"exception: {str(e)}",
                "qa_list": [],
                "input_tokens": 0,
                "output_tokens": 0,
                "total_tokens": 0
            }
            
            with open(failed_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(failed_entry, ensure_ascii=False) + '\n')
            
            # è®°å½•é”™è¯¯åˆ°æ—¥å¿—æ–‡ä»¶
            error_log_file = output_file.replace('.jsonl', '_errors.log')
            with open(error_log_file, 'a', encoding='utf-8') as f:
                f.write(f"Error processing item {i+1} (title: {title[:50]}): {e}\n")
            continue
        
        # æ¯å¤„ç†10æ¡è¯­æ–™æ‰“å°ä¸€æ¬¡è¯¦ç»†è¿›åº¦
        if (i + 1) % 10 == 0:
            print(f"\nğŸ“ˆ è¿›åº¦æŠ¥å‘Š:")
            print(f"   å·²å¤„ç†: {i + 1}/{len(corpus_data)} æ¡è¯­æ–™")
            print(f"   æˆåŠŸ: {i + 1 - processed_count - failed_count} æ¡")
            print(f"   å¤±è´¥: {failed_count} æ¡")
            print(f"   ç”ŸæˆQAå¯¹: {total_qa_count} ä¸ª")
            print(f"   Tokenä½¿ç”¨: è¾“å…¥={total_input_tokens}, è¾“å‡º={total_output_tokens}, æ€»è®¡={total_total_tokens}")
            if i + 1 - processed_count - failed_count > 0:
                print(f"   å¹³å‡æ¯æ¡æˆåŠŸè¯­æ–™: {total_qa_count/(i+1-processed_count-failed_count):.1f} ä¸ªQAå¯¹")
    
    print(f"\nğŸ‰ {dataset_name} æ•°æ®é›†å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"   æ€»è¯­æ–™æ•°: {len(corpus_data)} æ¡")
    print(f"   æˆåŠŸå¤„ç†: {len(corpus_data) - processed_count - failed_count} æ¡")
    print(f"   å¤±è´¥æ ·æœ¬: {failed_count} æ¡")
    print(f"   ç”ŸæˆQAå¯¹: {total_qa_count} ä¸ª")
    print(f"   Tokenä½¿ç”¨: è¾“å…¥={total_input_tokens}, è¾“å‡º={total_output_tokens}, æ€»è®¡={total_total_tokens}")
    if len(corpus_data) - processed_count - failed_count > 0:
        print(f"   å¹³å‡æ¯æ¡æˆåŠŸè¯­æ–™: {total_qa_count/(len(corpus_data)-processed_count-failed_count):.1f} ä¸ªQAå¯¹")
    print(f"   æˆåŠŸç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"   å¤±è´¥æ ·æœ¬å·²ä¿å­˜åˆ°: {failed_file}")

def process_all_corpora():
    """
    å¤„ç†æ‰€æœ‰ä¸‰ä¸ªè¯­æ–™æ–‡ä»¶
    """
    base_dir = "/root/autodl-tmp/ReadingCorpus/data/sampled"
    output_dir = "/root/autodl-tmp/ReadingCorpus/data/QA"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    # å®šä¹‰æ•°æ®é›†é…ç½®
    datasets = [
        {
            "name": "2wiki",
            "input_file": os.path.join(base_dir, "2wiki_sample_corpus.jsonl"),
            "output_file": os.path.join(output_dir, "2wiki_qa.jsonl")
        },
        # {
        #     "name": "hotpotqa", 
        #     "input_file": os.path.join(base_dir, "hotpotqa_sample_corpus.jsonl"),
        #     "output_file": os.path.join(output_dir, "hotpotqa_qa.jsonl")
        # },
        # {
        #     "name": "musique",
        #     "input_file": os.path.join(base_dir, "musique_sample_corpus.jsonl"),
        #     "output_file": os.path.join(output_dir, "musique_qa.jsonl")
        # },
        # {
        #     "name": "hotpotqa", 
        #     "input_file": os.path.join(base_dir, "hotpotqa_sample_corpus.jsonl"),
        #     "output_file": os.path.join(output_dir, "hotpotqa_qa.jsonl")
        # }
    ]
    
    # å¤„ç†æ¯ä¸ªæ•°æ®é›†
    for dataset in datasets:
        if os.path.exists(dataset["input_file"]):
            process_corpus_file(
                dataset["input_file"], 
                dataset["output_file"], 
                dataset["name"]
            )
        else:
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {dataset['input_file']}")

# --------- æµ‹è¯•å’Œä¸»ç¨‹åº ---------
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # æµ‹è¯•æ¨¡å¼
        chunk = "Alexander Fleming discovered penicillin in 1928 in London when he observed mold killing bacteria."
        result = generate_qa(chunk)
        print("æµ‹è¯•ç»“æœ:")
        print(json.dumps(result, indent=2, ensure_ascii=False))
    else:
        # å¤„ç†æ‰€æœ‰è¯­æ–™
        process_all_corpora()
